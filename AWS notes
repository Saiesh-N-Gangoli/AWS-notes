Cloud Computing
----------------
Cloud computing is a model for enabling ubiquitous, conveneient, on-demand network access to a shared pool of configurable computing resources that 
can be rapidly provisioned and released with minimal management effort or service provider interaction.(Defination given by National Institute of 
Standards and Technology).

Evolution of cloud computing:
-----------------------------
--> Dedicated servers: One physical machine was dedicated to one business.[Expensive, Maintainance cost and High Security].
--> Virtual Private Servers: One physical machine was dedicated to one business.But that physical machine is virtualized into various sub machines.
--> Shared Hosting: One physical machine is dedicated between multiple business. 
--> Cloud Hosting: Multiple physical machines act as one system.

Amazon Elastic Compute Cloud (EC2):
-----------------------------------
EC2 is virtual server. 

Deployment models:
------------------
1) On-premise
2) Cloud based
3) Hybrid cloud


1) On-premise:
-------------
On-premises deployment is also known as a private cloud deployment. In this model, resources are deployed on premises by using virtualization and 
resource management tools. For example, you might have applications that run on technology that is fully kept in your on-premises data center. 
Though this model is much like legacy IT infrastructure, its incorporation of application management and virtualization technologies helps to increase 
resource utilization.

2) Cloud:
---------
In a cloud-based deployment model, you can migrate existing applications to the cloud, or you can design and build new applications in the cloud. 
You can build those applications on low-level infrastructure that requires your IT staff to manage them. Alternatively, you can build them using higher-level 
services that reduce the management, architecting, and scaling requirements of the core infrastructure.For example, a company might create an application consisting 
of virtual servers, databases, and networking components that are fully based in the cloud.

3) Hybrid:
----------
In a hybrid deployment, cloud-based resources are connected to on-premises infrastructure. You might want to use this approach in a number of situations. 
For example, you have legacy applications that are better maintained on premises, or government regulations require your business to keep certain records on premises. 
For example, suppose that a company wants to use cloud services that can automate batch data processing and analytics. However, the company has several legacy applications 
that are more suitable on premises and will not be migrated to the cloud. With a hybrid deployment, the company would be able to keep the legacy applications on premises 
while benefiting from the data and analytics services that run in the cloud.

------------------------------------------------------------------------------------------------------------------------------------------------------------

Advantages of cloud computing:
------------------------------
1) No upfront cost.
2) Stop wasting money on data centers.
3) Stop guessing capacity of infrastructure.
4) Economies of scale.
5) Increase speed and agility.
6) Go global in minutes.

------------------------------------------------------------------------------------------------------------------------------------------------------------

How does the scale of cloud computing help you to save costs?

--> The aggregated cloud usage from a large number of customers results in lower pay-as-you-go prices.

------------------------------------------------------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------------------------------------------------------------
EC2: Amazon Elastic Compute Cloud (Amazon EC2) provides secure, resizable compute capacity in the cloud as Amazon EC2 instances. 
----
--> Flexible 
--> Cost effective
--> Quick

Multitenancy: Sharing underlying hardware between virtual machines.

------------------------------------------------------------------------------------------------------------------------------------------------------------
Amazon EC2 instance types:
--------------------------
--> General purpose: Balanced resources, diverse workloads, code repos, web servers.
--> Compute optimized: gaming, scientific modelling, HPC.
--> Memory optimized: Memory intensive tasks.
--> Storage optimized: High power for locally stored data.
--> Acclerated computing: Floating point number calculations, graphics processing, data pattern matching,utilize hardware accelarators.

If we map this back to our coffee shop, our cashier becomes a memory optimized EC2 instance, baristas become compute optimized instances, and our 
latte art employee is an accelerated computing instance type. And there you have it, EC2 instance types.

------------------------------------------------------------------------------------------------------------------------------------------------------------
EC2 Pricing:
------------
1) On-demand

2) Savings plan: Amazon EC2 Savings Plans enable you to reduce your compute costs by committing to a consistent amount of compute usage for a 1-year 
or 3-year term. This term commitment results in savings of up to 72% over On-Demand costs.

3) Reserved Instances: Predictable usage 1 to 3 years terms. All upfront, partial, no upfront. Reserved Instances are a billing discount applied to the 
use of On-Demand Instances in your account. You can purchase Standard Reserved and Convertible Reserved Instances for a 1-year or 3-year term, and 
Scheduled Reserved Instances for a 1-year term. You realize greater cost savings with the 3-year option.

At the end of a Reserved Instance term, you can continue using the Amazon EC2 instance without interruption. However, you are charged On-Demand rates 
until you do one of the following:

--> Terminate the instance.

--> Purchase a new Reserved Instance that matches the instance attributes (instance type, Region, tenancy, and platform).

There are 2 types:
  -> Standard Reserve Instances: Enables you to modify Availability Zone, scope, networking type, and instance size (within the same instance type) of Reserved Instance.
  -> Convertible Reserve Instances: Enables you to exchange one or more Convertible Reserved Instances for another Convertible Reserved Instance with a different configuration, 
  including instance family, operating system, and tenancy.

4) Spot Instances: Amazon EC2 Spot Instances let you take advantage of unused EC2 capacity in the AWS cloud. Spot Instances are available at up to a 90% discount compared to On-Demand prices. 
5) Dedicated Hosts: Physical hosts dedicated for your use for EC2. These are usually for meeting certain compliance requirements and nobody else will share tenancy of that host.

------------------------------------------------------------------------------------------------------------------------------------------------------------

Scaling Amazon EC2:
-------------------
Scalability involves beginning with only the resources you need and designing your architecture to automatically respond to changing demand by scaling out or in.

If you wanted the scaling process to happen automatically, which AWS service would you use? 
--> The AWS service that provides this functionality for Amazon EC2 instances is Amazon EC2 Auto Scaling.

--> Auto scaling is a horizontal scaling technique.

Amazon EC2 Auto Scaling enables you to automatically add or remove Amazon EC2 instances in response to changing application demand. By automatically scaling your instances in and out as needed, you are able to maintain a greater sense of application availability.

Within Amazon EC2 Auto Scaling, you can use two approaches: dynamic scaling and predictive scaling.

--> Dynamic scaling responds to changing demand. 

--> Predictive scaling automatically schedules the right number of Amazon EC2 instances based on predicted demand.

When you create an Auto Scaling group, you can set the minimum number of Amazon EC2 instances. The minimum capacity is the number of Amazon EC2 instances that launch immediately after you have created the Auto Scaling group. In this example, the Auto Scaling group has a minimum capacity of one Amazon EC2 instance. Next, you can set the desired capacity at two Amazon EC2 instances even though your application needs a minimum of a single Amazon EC2 instance to run. The third configuration that you can set in an Auto Scaling group is the maximum capacity. For example, you might configure the Auto Scaling group to scale out in response to increased demand, but only to a maximum of four Amazon EC2 instances.

Note: If you do not specify the desired number of Amazon EC2 instances in an Auto Scaling group, the desired capacity defaults to your minimum capacity.

------------------------------------------------------------------------------------------------------------------------------------------------------------

Elastic Load Balancing:
-----------------------
--> Elastic Load Balancing is the AWS service that automatically distributes incoming application traffic across multiple resources, such as Amazon EC2 instances. 

--> A load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group. 

--> This means that as you add or remove Amazon EC2 instances in response to the amount of incoming traffic, these requests route to the load balancer first. 

--> Then, the requests spread across multiple resources that will handle them. For example, if you have multiple Amazon EC2 instances, Elastic Load Balancing distributes the workload across the multiple instances so that no single instance has to carry the bulk of it. 

------------------------------------------------------------------------------------------------------------------------------------------------------------

Messaging and Queuing:
----------------------

Monolithic Applications:
------------------------
Suppose that you have an application with tightly coupled components. These components might include databases, servers, the user interface, business logic, and so on. This type of architecture can be considered a monolithic application. 

Microservices:
--------------
In a microservices approach, application components are loosely coupled. In this case, if a single component fails, the other components continue to work because they are communicating with each other. The loose coupling prevents the entire application from failing. 

When designing applications on AWS, you can take a microservices approach with services and components that fulfill different functions. Two services facilitate application integration: Amazon Simple Notification Service (Amazon SNS) and Amazon Simple Queue Service (Amazon SQS).

1) Amazon Simple Notification Service [SNS]:
--------------------------------------------
Amazon Simple Notification Service (Amazon SNS) is a publish/subscribe service. Using Amazon SNS topics, a publisher publishes messages to subscribers. It can used to send, receive and store messages but the additonal feature it provides is that is sends notification to the end subscriber.

Message ----> SNS Topic ----- sub1, sub2, sub3, ....

SNS Topic: 
----------
SNS topic is the channel to deliver messages.

2) Amazon Simple Queue Service:
-------------------------------
Amazon Simple Queue Service (Amazon SQS) is a message queuing service. Using Amazon SQS, you can send, store, and receive messages between software components, without losing messages or requiring other services to be available. In Amazon SQS, an application sends messages into a queue. A user or service retrieves a message from the queue, processes it, and then deletes it from the queue.

------------------------------------------------------------------------------------------------------------------------------------------------------------

Serverless Computing:
---------------------
The term “serverless” means that your code runs on servers, but you do not need to provision or manage these servers. With serverless computing, you can focus more on innovating new products and features instead of maintaining servers.

AWS Lambda:
-----------
AWS Lambda is a service that lets you run code without needing to provision or manage servers. While using AWS Lambda, you pay only for the compute time that you consume. Charges apply only when your code is running. You can also run code for virtually any type of application or backend service, all with zero administration. For example, a simple Lambda function might involve automatically resizing uploaded images to the AWS Cloud. In this case, the function triggers when uploading a new image. 

Steps:
------
1) You upload your code to Lambda. 

2) You set your code to trigger from an event source, such as AWS services, mobile applications, or HTTP endpoints.

3) Lambda runs your code only when triggered.

4) You pay only for the compute time that you use. 

Containers:
-----------
Containers provide you with a standard way to package your application's code and dependencies into a single object. 

There are two type of Container Services:
-----------------------------------------
1) Amazon Elastic Container Service
2) Amazon Elastic Kubernetes Service

Amazon Elastic Container Service (Amazon ECS)
----------------------------------------------
ECS is a highly scalable, high-performance container management system that enables you to run and scale containerized applications on AWS. 

Amazon ECS supports Docker containers.
 
Docker is a software platform that enables you to build, test, and deploy applications quickly. AWS supports the use of open-source Docker Community Edition and subscription-based Docker Enterprise Edition. With Amazon ECS, you can use API calls to launch and stop Docker-enabled applications.

Both ECS and EKS run on EC2 instance. If we don't need EC2 instance we can use AWS Fargate.

--> Fargate helps in running ECS and EKS without an EC2 instance.

--> So developing an traditonal application which is huge needs an EC2 instance but developing small applications can be done through fargate.

Amazon Elastic Kubernetes Service (Amazon ECS)
----------------------------------------------
Amazon Elastic Kubernetes Service (Amazon EKS) is a managed Kubernetes service to run Kubernetes in the AWS cloud and on-premises data centers. In the cloud, Amazon EKS automatically manages the availability and scalability of the Kubernetes control plane nodes responsible for scheduling containers, managing application availability, storing cluster data, and other key tasks.

------------------------------------------------------------------------------------------------------------------------------------------------------------

AWS Global Infrastructure:
--------------------------

Regions:
--------
Factors on which regions depend upon:
-------------------------------------
--> Compilance: Compliance means how the government in which the regions are present is making rules and how they needs to be obeyed. Some government doesnt allow you to keep the data out of certain region and in such cases we need to keep compliance in mind.

--> Proximity: Proximity means how close the businesses customers are closer to the region. If the customers of business are closer to a certain AWS region then we need have our region closer to the customers so that the speed of data retrival is fast.

--> Feature Availability: All the features are not available in all the regions. So certain features are available only in certain regions and in that condition we need to plan the regions according to our need.

--> Pricing: Pricing plays an pivtol role in the factors of regions. Regions pricing differ from region to region though they have same features available. Some government bodies take huge amount taxes and that might effect this. So business should concentrate on the pricing as well.

Example:
--------
The way Brazil’s tax structure is set up, it might cost 50% more to run the same workload out of the São Paulo Region compared to the Oregon Region.

------------------------------------------------------------------------------------------------------------------------------------------------------------

Availability Zones:
-------------------
Group of data centers in a region is called AVAILABILITY ZONES.

An Availability Zone is a single data center or a group of data centers within a Region. Availability Zones are located tens of miles apart from each other. This is close enough to have low latency (the time between when content requested and received) between Availability Zones. However, if a disaster occurs in one part of the Region, they are distant enough to reduce the chance that multiple Availability Zones are affected.

--> A best practice is to run applications across at least two Availability Zones in a Region.

------------------------------------------------------------------------------------------------------------------------------------------------------------

Edge Locations:
---------------
Suppose our business runs in Bangalore. But our data center is in Brazil. It will be difficult for the people of Bangalore to communicate with the Brazil's data center as the latency might increase and slow downs the process. So we can have Content Delivery Network[CDNs] which will have the cache or local copy of your data nearer to your location itself. These locations are called as Edge Locations.

--> AWS's CDN is called AMAZON CLOUD FRONT

So we can define an edge location as a site that Amazon CloudFront uses to store cached copies of your content closer to your customers for faster delivery.

AWS Outposts:
-------------
Having these AWS infrastrctures in the business building is called as AWS Outposts. Its basically like On-premise.

------------------------------------------------------------------------------------------------------------------------------------------------------------

Provisioning AWS Resources:
---------------------------
There are majorly 3 ways to provision the AWS resources:
--------------------------------------------------------
1) AWS Management Console

2) AWS Commond Line Interface(CLI)

3) AWS Software Developement Kits(SDKs)

AWS Management Console
----------------------
AWS Management Console can be used for running test envinornments, viewing AWS Bills, Monitoring views, viewing alarms and to work with non technical resources.

AWS CLI:
--------
To save time when making API requests, you can use the AWS Command Line Interface (AWS CLI). AWS CLI enables you to control multiple AWS services directly from the command line within one tool. AWS CLI is available for users on Windows, macOS, and Linux. 

By using AWS CLI, you can automate the actions that your services and applications perform through scripts. For example, you can use commands to launch an Amazon EC2 instance, connect an Amazon EC2 instance to a specific Auto Scaling group, and more.

AWS SDKs:
---------
SDKs make it easier for you to use AWS services through an API designed for your programming language or platform. SDKs enable you to use AWS services with your existing applications or create entirely new applications that will run on AWS. To help you get started with using SDKs, AWS provides documentation and sample code for each supported programming language. Supported programming languages include C++, Java, .NET, and more.

There are other Manage Tools for provisioning AWS Resources:
------------------------------------------------------------

1) AWS Elastic BeanStalk:
-------------------------
Here you need to provide the application code and desired configurations and the beanstalk will build the instance for you. We can save this configurations for the future use also.

With AWS Elastic Beanstalk, you provide code and configuration settings, and Elastic Beanstalk deploys the resources necessary to perform the following tasks:

--> Adjust capacity

--> Load balancing

--> Automatic scaling

--> Application health monitoring

2) AWS CloudFormation:
-----------------------
AWS Cloudformation will help you in building the instance by setting the configurations in JSON or YARN format. We need to only concentrate on what we want to build and not how to build it. Cloudformation has large impact in the field of Storage, DB, ML and Analytics.

--> With AWS CloudFormation, you can treat your infrastructure as code. This means that you can build an environment by writing lines of code instead of using the AWS Management Console to individually provision resources.

--> AWS CloudFormation provisions your resources in a safe, repeatable manner, enabling you to frequently build your infrastructure and applications without having to perform manual actions or write custom scripts. It determines the right operations to perform when managing your stack and rolls back changes automatically if it detects errors.

------------------------------------------------------------------------------------------------------------------------------------------------------------

NETWORKING
----------
Amazon Virtual Private Cloud (Amazon VPC)
-----------------------------------------
Amazon VPC enables you to provision an isolated section of the AWS Cloud. In this isolated section, you can launch resources in a virtual network that you define. Within a virtual private cloud (VPC), you can organize your resources into subnets. A subnet is a section of a VPC that can contain resources such as Amazon EC2 instances.

Internet gateway
----------------
To allow public traffic from the internet to access your VPC, you attach an internet gateway to the VPC.

An internet gateway is a connection between a VPC and the internet. You can think of an internet gateway as being similar to a doorway that customers use to enter the coffee shop. Without an internet gateway, no one can access the resources within your VPC.

Virtual Private Gateway
-----------------------
Here’s an example of how a virtual private gateway works. You can think of the internet as the road between your home and the coffee shop. Suppose that you are traveling on this road with a bodyguard to protect you. You are still using the same road as other customers, but with an extra layer of protection. 

The bodyguard is like a virtual private network (VPN) connection that encrypts (or protects) your internet traffic from all the other requests around it. 

The virtual private gateway is the component that allows protected internet traffic to enter into the VPC. Even though your connection to the coffee shop has extra protection, traffic jams are possible because you’re using the same road as other customers. 

--> A virtual private gateway enables you to establish a virtual private network (VPN) connection between your VPC and a private network, such as an on-premises data center or internal corporate network. A virtual private gateway allows traffic into the VPC only if it is coming from an approved network.

AWS Direct Connect
------------------
AWS Direct Connect is a service that enables you to establish a dedicated private connection between your data center and a VPC.  

Suppose that there is an apartment building with a hallway directly linking the building to the coffee shop. Only the residents of the apartment building can travel through this hallway. This private hallway provides the same type of dedicated connection as AWS Direct Connect. Residents are able to get into the coffee shop without needing to use the public road shared with other customers. 

Subnets
-------
A subnet is a section of a VPC in which you can group resources based on security or operational needs. Subnets can be public or private.

--> Public subnets contain resources that need to be accessible by the public, such as an online store’s website.

--> Private subnets contain resources that should be accessible only through your private network, such as a database that contains customers’ personal information and order histories.  

--> In a VPC, subnets can communicate with each other. For example, you might have an application that involves Amazon EC2 instances in a public subnet communicating with databases that are located in a private subnet.

Network Traffic in a VPC
------------------------
When a customer requests data from an application hosted in the AWS Cloud, this request is sent as a packet. A packet is a unit of data sent over the internet or a network. 

It enters into a VPC through an internet gateway. Before a packet can enter into a subnet or exit from a subnet, it checks for permissions. These permissions indicate who sent the packet and how the packet is trying to communicate with the resources in a subnet.

The VPC component that checks packet permissions for subnets is a network access control list (ACL).

Network Access Control Lists (ACLs)
------------------------------------
A network access control list (ACL) is a virtual firewall that controls inbound and outbound traffic at the subnet level.

For example, step outside of the coffee shop and imagine that you are in an airport. In the airport, travelers are trying to enter into a different country. You can think of the travelers as packets and the passport control officer as a network ACL. The passport control officer checks travelers’ credentials when they are both entering and exiting out of the country. If a traveler is on an approved list, they are able to get through. However, if they are not on the approved list or are explicitly on a list of banned travelers, they cannot come in.

Each AWS account includes a default network ACL. When configuring your VPC, you can use your account’s default network ACL or create custom network ACLs. 

By default, your account’s default network ACL allows all inbound and outbound traffic, but you can modify it by adding your own rules. For custom network ACLs, all inbound and outbound traffic is denied until you add rules to specify which traffic to allow. Additionally, all network ACLs have an explicit deny rule. This rule ensures that if a packet doesn’t match any of the other rules on the list, the packet is denied. 

Stateless Packet Filtering
--------------------------
Network ACLs perform stateless packet filtering. They remember nothing and check packets that cross the subnet border each way: inbound and outbound. 

When a packet response for that request comes back to the subnet, the network ACL does not remember your previous request. The network ACL checks the packet response against its list of rules to determine whether to allow or deny.

Security Groups
---------------
A security group is a virtual firewall that controls inbound and outbound traffic for an Amazon EC2 instance.

By default, a security group denies all inbound traffic and allows all outbound traffic. You can add custom rules to configure which traffic to allow or deny.

Stateful Packet Filtering
--------------------------
Security groups perform stateful packet filtering. They remember previous decisions made for incoming packets.

Consider the same example of sending a request out from an Amazon EC2 instance to the internet. When a packet response for that request returns to the instance, the security group remembers your previous request. The security group allows the response to proceed, regardless of inbound security group rules.

Global Networking:
------------------

Domain Name System (DNS)
-------------------------
Suppose that AnyCompany has a website hosted in the AWS Cloud. Customers enter the web address into their browser, and they are able to access the website. This happens because of Domain Name System (DNS) resolution. DNS resolution involves a DNS server communicating with a web server.

For example, suppose that you want to visit AnyCompany’s website. 

1) When you enter the domain name into your browser, this request is sent to a DNS server. 
2) The DNS server asks the web server for the IP address that corresponds to AnyCompany’s website.
3) The web server responds by providing the IP address for AnyCompany’s website, 192.0.2.0.

Amazon Route 53
---------------
Amazon Route 53 is a DNS web service. It gives developers and businesses a reliable way to route end users to internet applications hosted in AWS. The steps how the website of Amazon Cloud works is:

1 --> A customer requests data from the application by going to AnyCompany’s website. 

2 --> Amazon Route 53 uses DNS resolution to identify AnyCompany.com’s corresponding IP address, 192.0.2.0. This information is sent back to the customer. 

3 --> The customer’s request is sent to the nearest edge location through Amazon CloudFront. 

4 --> Amazon CloudFront connects to the Application Load Balancer, which sends the incoming packet to an Amazon EC2 instance.

------------------------------------------------------------------------------------------------------------------------------------------------------------

Storage and Databases:
----------------------

Instance stores
---------------
An instance store provides temporary block-level storage for an Amazon EC2 instance. An instance store is disk storage that is physically attached to the host computer for an EC2 instance, and therefore has the same lifespan as the instance. When the instance is terminated, you lose any data in the instance store.

Amazon Elastic Block Storage (Amazon EBS)
-----------------------------------------
Amazon Elastic Block Store (Amazon EBS)
 is a service that provides block-level storage volumes that you can use with Amazon EC2 instances. If you stop or terminate an Amazon EC2 instance, all the data on the attached EBS volume remains available.

--> To create an EBS volume, you define the configuration (such as volume size and type) and provision it. After you create an EBS volume, it can attach to an Amazon EC2 instance.

--> Because EBS volumes are for data that needs to persist, it’s important to back up the data. You can take incremental backups of EBS volumes by creating Amazon EBS snapshots.

Amazon EBS Snapshots
---------------------
An EBS snapshot is an incremental backup. This means that the first backup taken of a volume copies all the data. For subsequent backups, only the blocks of data that have changed since the most recent snapshot are saved. 

------------------------------------------------------------------------------------------------------------------------------------------------------------

Object Storage
--------------
In object storage, each object consists of data, metadata, and a key.

Amazon Simple Storage Service(S3)
---------------------------------
Amazon Simple Storage Service (Amazon S3) is a service that provides object-level storage. Amazon S3 stores data as objects in buckets. You can upload any type of file to Amazon S3, such as images, videos, text files, and so on. For example, you might use Amazon S3 to store backup files, media files for a website, or archived documents. Amazon S3 offers unlimited storage space. The maximum file size for an object in Amazon S3 is 5 TB. When you upload a file to Amazon S3, you can set permissions to control visibility and access to it. You can also use the Amazon S3 versioning feature to track changes to your objects over time.

Amazon S3 Storage Classes
-------------------------
With Amazon S3, you pay only for what you use. You can choose from a range of storage classes to select a fit for your business and cost needs. When selecting an Amazon S3 storage class, consider these two factors:

--> How often you plan to retrieve your data
--> How available you need your data to be

Amazon S3 Standard
------------------
1) Designed for frequently accessed data
2) Stores data in a minimum of three Availability Zones

Amazon S3 Standard provides high availability for objects. This makes it a good choice for a wide range of use cases, such as websites, content distribution, and data analytics. Amazon S3 Standard has a higher cost than other storage classes intended for infrequently accessed data and archival storage.

Amazon S3 Standard-Infrequent Access (S3 Standard-IA)
-----------------------------------------------------
1) Ideal for infrequently accessed data
2) Similar to Amazon S3 Standard but has a lower storage price and higher retrieval price

Amazon S3 Standard-IA is ideal for data infrequently accessed but requires high availability when needed. Both Amazon S3 Standard and Amazon S3 Standard-IA store data in a minimum of three Availability Zones. S3 Standard-IA provides the same level of availability as Amazon S3 Standard but with a lower storage price and a higher retrieval price.

Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)
-----------------------------------------------------
1) Stores data in a single Availability Zone
2) Has a lower storage price than Amazon S3 Standard-IA

Compared to Amazon S3 Standard and Amazon S3 Standard-IA, which store data in a minimum of three Availability Zones, Amazon S3 One Zone-IA stores data in a single Availability Zone. This makes it a good storage class to consider if the following conditions apply:

--> You want to save costs on storage.
--> You can easily reproduce your data in the event of an Availability Zone failure.

Amazon S3 Intelligent-Tiering
-----------------------------
1) Ideal for data with unknown or changing access patterns.
2) Requires a small monthly monitoring and automation fee per object

In the Amazon S3 Intelligent-Tiering storage class, Amazon S3 monitors objects’ access patterns. If you haven’t accessed an object for 30 consecutive days, Amazon S3 automatically moves it to the infrequent access tier, Amazon S3 Standard-IA. If you access an object in the infrequent access tier, Amazon S3 automatically moves it to the frequent access tier, Amazon S3 Standard.

Amazon S3 Glacier Instant Retrieval
-----------------------------------
1) Works well for archived data that requires immediate access
2) Can retrieve objects within a few milliseconds

When you decide between the options for archival storage, consider how quickly you must retrieve the archived objects. You can retrieve objects stored in the Amazon S3 Glacier Instant Retrieval storage class within milliseconds, with the same performance as Amazon S3 Standard.

Amazon S3 Glacier Flexible Retrieval
------------------------------------
1) Low-cost storage designed for data archiving
2) Able to retrieve objects within a few minutes to hours

Amazon S3 Glacier Flexible Retrieval is a low-cost storage class that is ideal for data archiving. For example, you might use this storage class to store archived customer records or older photos and video files.

Amazon S3 Glacier Deep Archive
-------------------------------
1) Lowest-cost object storage class ideal for archiving
2) Able to retrieve objects within 12 hours

Amazon S3 Deep Archive supports long-term retention and digital preservation for data that might be accessed once or twice in a year. This storage class is the lowest-cost storage in the AWS Cloud, with data retrieval from 12 to 48 hours. All objects from this storage class are replicated and stored across at least three geographically dispersed Availability Zones.

Amazon S3 Outposts
------------------
1) Creates S3 buckets on Amazon S3 Outposts
2) Makes it easier to retrieve, store, and access data on AWS Outposts

Amazon S3 Outposts delivers object storage to your on-premises AWS Outposts environment. Amazon S3 Outposts is designed to store data durably and redundantly across multiple devices and servers on your Outposts. It works well for workloads with local data residency requirements that must satisfy demanding performance needs by keeping data close to on-premises applications.

AWS EBS vs AWS S3
-----------------

Amazon S3:
---------- 
Amazon S3 is a simple storage service offered by Amazon and it is useful for hosting website images and videos, data analytics, etc. S3 is an object-level data storage that distributes the data objects across several machines and allows the users to access the storage via the internet from any corner of the world.

Amazon EBS: 
-----------
Unlike Amazon S3, Amazon EBS is a block-level data storage offered by Amazon. Block storage stores files in multiple volumes called blocks, which act as separate hard drives, and this storage is not accessible via the internet. Use cases include business continuity, transactional and NO SQL database, software testing, etc..

*) Storage type:
----------------
--> S3 provides object storage.
--> EBS provides block storage.


*) Accessibility:
-----------------
-> The files within an S3 bucket are stored in an unstructured manner and can be retrieved using HTTP protocols but the data stored in EBS is only accessible by the instance to which it is connected to.


*) Durability:
--------------
Amazon S3 provides durability by redundantly storing the data across multiple Availability Zones whereas EBS provides durability by redundantly storing the data in a single Availability Zone.

*) Size of data:
----------------
S3 can store large amounts as compared to EBS. With S3, the standard limit is of 100 buckets and each bucket has got an unlimited data capacity whereas EBS has a standard limit of 20 volumes and each volume can hold data up to 1TB. In EBS there occurs an upper limit on the data storage.

*) Performance:
--------------
Amazon EBS is faster storage and offers high performance as compared to S3.

Amazon Elastic File System (Amazon EFS):
----------------------------------------
EFS is a scalable file system used with AWS Cloud services and on-premises resources. As you add and remove files, Amazon EFS grows and shrinks automatically. It can scale on demand to petabytes without disrupting applications. 

EBS vs EFS:
-----------
--> An Amazon EBS volume stores data in a single Availability Zone. 
--> To attach an Amazon EC2 instance to an EBS volume, both the Amazon EC2 instance and the EBS volume must reside within the same Availability Zone.

--> Amazon EFS is a regional service. It stores data in and across multiple Availability Zones. 
--> The duplicate storage enables you to access data concurrently from all the Availability Zones in the Region where a file system is located. 

Amazon Relational Database Service (Amazon RDS)
-----------------------------------------------
Relational databases use structured query language (SQL) to store and query data. This approach allows data to be stored in an easily understandable, consistent, and scalable way. 

Amazon Relational Database Service
----------------------------------
Amazon Relational Database Service (Amazon RDS) is a service that enables you to run relational databases in the AWS Cloud. Amazon RDS is a managed service that automates tasks such as hardware provisioning, database setup, patching, and backups. With these capabilities, you can spend less time completing administrative tasks and more time using data to innovate your applications. You can integrate Amazon RDS with other services to fulfill your business and operational needs, such as using AWS Lambda to query your database from a serverless application.

Amazon RDS is available on six database engines, which optimize for memory, performance, or input/output (I/O). Supported database engines include:

--> Amazon Aurora
--> PostgreSQL
--> MySQL
--> MariaDB
--> Oracle Database
--> Microsoft SQL Server

Amazon Aurora
-------------
Amazon Aurora is an enterprise-class relational database. It is compatible with MySQL and PostgreSQL relational databases. It is up to five times faster than standard MySQL databases and up to three times faster than standard PostgreSQL databases.

--> Amazon Aurora helps to reduce your database costs by reducing unnecessary input/output (I/O) operations.
--> Ensures that your database resources remain reliable and available. 
--> Consider Amazon Aurora if your workloads require high availability. 
--> It replicates six copies of your data across three Availability Zones.
--> Continuously backs up your data to Amazon S3.

Nonrelational Databases
-----------------------
In a nonrelational database, you create tables. A table is a place where you can store and query data.

--> Nonrelational databases are sometimes referred to as “NoSQL databases” because they use structures other than rows and columns to organize data. 
--> One type of structural approach for nonrelational databases is key-value pairs. 
--> With key-value pairs, data is organized into items (keys), and items have attributes (values). 

Amazon DynamoDB
---------------
Amazon DynamoDB
Amazon DynamoDB
 is a key-value database service. It delivers single-digit millisecond performance at any scale.

Serverless
----------
--> DynamoDB is serverless, which means that you do not have to provision, patch, or manage servers. 
--> You also do not have to install, maintain, or operate software.

Automatic Scaling
-----------------
--> As the size of your database shrinks or grows, DynamoDB automatically scales to adjust for changes in capacity while maintaining consistent performance. 
--> This makes it a suitable choice for use cases that require high performance while scaling.

Amazon Redshift
---------------
Amazon Redshift is a data warehousing service that you can use for big data analytics. It offers the ability to collect data from many sources and helps you to understand relationships and trends across your data.

AWS Database Migration Service (AWS DMS)
----------------------------------------
AWS Database Migration Service (AWS DMS) enables you to migrate relational databases, nonrelational databases, and other types of data stores.

With AWS DMS, you move data between a source database and a target database. The source and target databases can be of the same type or different types. During the migration, your source database remains operational, reducing downtime for any applications that rely on the database. 

Other use cases for AWS DMS:
----------------------------

--> Development and test database migrations:
---------------------------------------------
Enabling developers to test applications against production data without affecting production users.

--> Database consolidation:
---------------------------
Combining several databases into a single database.

--> Continuous replication:
---------------------------
Sending ongoing copies of your data to other target sources instead of doing a one-time migration.


Additional Database Services
----------------------------

Amazon DocumentDB
-----------------
Amazon DocumentDB is a document database service that supports MongoDB workloads. (MongoDB is a document database program.)

Amazon Neptune
--------------
Amazon Neptune is a graph database service. You can use Amazon Neptune to build and run applications that work with highly connected datasets, such as recommendation engines, fraud detection, and knowledge graphs.

Amazon Quantum Ledger Database (Amazon QLDB)
-------------------------------------------- 
Amazon Quantum Ledger Database (Amazon QLDB) is a ledger database service. 
--> You can use Amazon QLDB to review a complete history of all the changes that have been made to your application data.

Amazon Managed Blockchain
-------------------------
Amazon Managed Blockchain is a service that you can use to create and manage blockchain networks with open-source frameworks. 
--> Blockchain is a distributed ledger system that lets multiple parties run transactions and share data without a central authority.

Amazon ElastiCache
------------------
Amazon ElastiCache is a service that adds caching layers on top of your databases to help improve the read times of common requests. 
--> It supports two types of data stores: Redis and Memcached.

Amazon DynamoDB Accelerator
---------------------------
Amazon DynamoDB Accelerator (DAX) is an in-memory cache for DynamoDB. 
--> It helps improve response times from single-digit milliseconds to microseconds.

------------------------------------------------------------------------------------------------------------------------------------------------------------

Security:
---------

Shared Responsibility Model
---------------------------

Customers: Security in the Cloud
---------------------------------
--> Customers are responsible for the security of everything that they create and put in the AWS Cloud.
--> When using AWS services, you, the customer, maintain complete control over your content. 
--> You are responsible for managing security requirements for your content.
--> Including which content you choose to store on AWS.
--> Which AWS services you use. 
--> Who has access to that content. 
--> You also control how access rights are granted, managed, and revoked.

"The security steps that you take will depend on factors such as the services that you use, the complexity of your systems, and your company’s specific operational and security needs. Steps include selecting, configuring, and patching the operating systems that will run on Amazon EC2 instances, configuring security groups, and managing user accounts. "

AWS: Security of the Cloud
--------------------------
--> AWS is responsible for security of the cloud.
--> AWS operates, manages, and controls the components at all layers of infrastructure. 
--> This includes areas such as the host operating system, the virtualization layer, and even the physical security of the data centers from which services operate. 

--> AWS is responsible for protecting the global infrastructure that runs all of the services offered in the AWS Cloud. 
--> This infrastructure includes AWS Regions, Availability Zones, and edge locations. 

User Permission and Access
--------------------------
AWS Identity and Access Management (IAM)
----------------------------------------
--> AWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely.   
--> IAM gives you the flexibility to configure access based on your company’s specific operational and security needs. 

AWS Account Root User
---------------------
--> When you first create an AWS account, you begin with an identity known as the root user. 
--> The root user is accessed by signing in with the email address and password that you used to create your AWS account. 
--> It has complete access to all the AWS services and resources in the account.

IAM users
---------
--> An IAM user is an identity that you create in AWS. 
--> It represents the person or application that interacts with AWS services and resources. 
--> It consists of a name and credentials.
--> By default, when you create a new IAM user in AWS, it has no permissions associated with it. 
--> To allow the IAM user to perform specific actions in AWS, such as launching an Amazon EC2 instance or creating an Amazon S3 bucket, you must grant the IAM user the necessary permissions.

IAM policies
------------
--> An IAM policy is a document that allows or denies permissions to AWS services and resources.  
--> IAM policies enable you to customize users levels of access to resources. 
--> For example, you can allow users to access all of the Amazon S3 buckets within your AWS account, or only a specific bucket.

IAM Groups
----------
--> An IAM group is a collection of IAM users. 
--> When you assign an IAM policy to a group, all users in the group are granted permissions specified by the policy.

IAM Roles:
----------
--> An IAM role is an identity that you can assume to gain temporary access to permissions.  
--> Before an IAM user, application, or service can assume an IAM role, they must be granted permissions to switch to the role. 
--> When someone assumes an IAM role, they abandon all previous permissions that they had under a previous role and assume the permissions of the new role.

Multi-factor Authentication
---------------------------
--> First, when a user signs in to an AWS website, they enter their IAM user ID and password. 
--> Next, the user is prompted for an authentication response from their AWS MFA device. 
--> This device could be a hardware security key, a hardware device, or an MFA application on a device such as a smartphone.
--> When the user has been successfully authenticated, they are able to access the requested AWS services or resources.

------------------------------------------------------------------------------------------------------------------------------------------------------------

AWS Organizations
-----------------
--> Suppose that your company has multiple AWS accounts you can use AWS Organizations.
--> This helps in consolidating and managing multiple AWS accounts within a central location.
--> AWS Organizations automatically creates a root, which is the parent container for all the accounts in your organization. 
--> In AWS Organizations, you can centrally control permissions for the accounts in your organization by using service control policies (SCPs).
--> SCPs enable you to place restrictions on the AWS services, resources, and individual API actions that users and roles in each account can access.
--> Consolidated billing is another feature of AWS Organizations. You will learn about consolidated billing in a later module.

Organizational Units
--------------------
In AWS Organizations, you can group accounts into organizational units (OUs) to make it easier to manage accounts with similar business or security requirements. When you apply a policy to an OU, all the accounts in the OU automatically inherit the permissions specified in the policy.  

Compliance:
-----------

AWS Artifact
------------
Depending on your company’s industry, you may need to uphold specific standards. An audit or inspection will ensure that the company has met those standards. AWS Artifact is a service that provides on-demand access to AWS security and compliance reports and select online agreements. AWS Artifact consists of two main sections: AWS Artifact Agreements and AWS Artifact Reports.

AWS Artifact Agreements
-----------------------
Suppose that your company needs to sign an agreement with AWS regarding your use of certain types of information throughout AWS services. You can do this through AWS Artifact Agreements. In AWS Artifact Agreements, you can review, accept, and manage agreements for an individual account and for all your accounts in AWS Organizations. Different types of agreements are offered to address the needs of customers who are subject to specific regulations, such as the Health Insurance Portability and Accountability Act (HIPAA).

AWS Artifact Reports
--------------------
Next, suppose that a member of your company’s development team is building an application and needs more information about their responsibility for complying with certain regulatory standards. You can advise them to access this information in AWS Artifact Reports. 

Denial-of-Service Attacks
-------------------------
A denial-of-service (DoS) attack is a deliberate attempt to make a website or application unavailable to users.

--> For example, an attacker might flood a website or application with excessive network traffic until the targeted website or application becomes overloaded and is no longer able to respond. If the website or application becomes unavailable, this denies service to users who are trying to make legitimate requests.

--> This happens from single source.

--> Distributed Denial-of-Service Attacks
-----------------------------------------
In a distributed denial-of-service (DDoS) attack, multiple sources are used to start an attack that aims to make a website or application unavailable. This can come from a group of attackers, or even a single attacker. The single attacker can use multiple infected computers (also known as “bots”) to send excessive traffic to a website or application.

AWS Shield
----------
AWS Shield is a service that protects applications against DDoS attacks. AWS Shield provides two levels of protection: Standard and Advanced.

AWS Shield Standard
--------------------
AWS Shield Standard automatically protects all AWS customers at no cost. It protects your AWS resources from the most common, frequently occurring types of DDoS attack. As network traffic comes into your applications, AWS Shield Standard uses a variety of analysis techniques to detect malicious traffic in real time and automatically mitigates it. 

AWS Shield Advanced
-------------------
AWS Shield Advanced is a paid service that provides detailed attack diagnostics and the ability to detect and mitigate sophisticated DDoS attacks. It also integrates with other services such as Amazon CloudFront, Amazon Route 53, and Elastic Load Balancing. Additionally, you can integrate AWS Shield with AWS WAF by writing custom rules to mitigate complex DDoS attacks.

Additional Security Services
----------------------------

AWS Key Management Service (AWS KMS)
------------------------------------
AWS Key Management Service (AWS KMS) enables you to perform encryption operations through the use of cryptographic keys. A cryptographic key is a random string of digits used for locking (encrypting) and unlocking (decrypting) data. You can use AWS KMS to create, manage, and use cryptographic keys. You can also control the use of keys across a wide range of services and in your applications.

AWS WAF
-------
AWS WAF is a web application firewall that lets you monitor network requests that come into your web applications. 

Amazon Inspector
----------------
Amazon Inspector helps to improve the security and compliance of applications by running automated security assessments. It checks applications for security vulnerabilities and deviations from security best practices, such as open access to Amazon EC2 instances and installations of vulnerable software versions. 
After Amazon Inspector has performed an assessment, it provides you with a list of security findings. The list prioritizes by severity level, including a detailed description of each security issue and a recommendation for how to fix it. However, AWS does not guarantee that following the provided recommendations resolves every potential security issue. Under the shared responsibility model, customers are responsible for the security of their applications, processes, and tools that run on AWS services. is a

Amazon GuardDuty
----------------
Amazon GuardDuty is a service that provides intelligent threat detection for your AWS infrastructure and resources. It identifies threats by continuously monitoring the network activity and account behavior within your AWS environment.

------------------------------------------------------------------------------------------------------------------------------------------------------------

Monitoring:
-----------

Amazon CloudWatch
-----------------
Amazon CloudWatch(opens in a new tab) is a web service that enables you to monitor and manage various metrics and configure alarm actions based on data from those metrics.CloudWatch uses metrics(opens in a new tab) to represent the data points for your resources. AWS services send metrics to CloudWatch. CloudWatch then uses these metrics to create graphs automatically that show how performance has changed over time. 

CloudWatch alarms
-----------------
With CloudWatch, you can create alarms(opens in a new tab) that automatically perform actions if the value of your metric has gone above or below a predefined threshold. 

AWS CloudTrail
--------------
--> CloudTrail records API calls for your account. 
--> Recorded information includes the identity of the API caller, the time of the API call, the source IP address of the API caller, and more. 
--> You can think of CloudTrail as a “trail” of breadcrumbs (or a log of actions) that someone has left behind them.
--> Recall that you can use API calls to provision, manage, and configure your AWS resources. 
--> With CloudTrail, you can view a complete history of user activity and API calls for your applications and resources. 
--> Events are typically updated in CloudTrail within 15 minutes after an API call. 
--> You can filter events by specifying the time and date that an API call occurred, the user who requested the action, the type of resource that was involved in the API call, and more.

CloudTrail Insights
-------------------
--> Within CloudTrail, you can also enable CloudTrail Insights
--> This optional feature allows CloudTrail to automatically detect unusual API activities in your AWS account.

AWS Trusted Advisor
-------------------
AWS Trusted Advisor is a web service that inspects your AWS environment and provides real-time recommendations in accordance with AWS best practices.

Trusted Advisor compares its findings to AWS best practices in five categories: 
--> Cost optimization
--> Performance
--> Security
--> Fault tolerance
--> Service limits. 

For the checks in each category, Trusted Advisor offers a list of recommended actions and additional resources to learn more about AWS best practices. 

--> The guidance provided by AWS Trusted Advisor can benefit your company at all stages of deployment. 
For example, you can use AWS Trusted Advisor to assist you while you are creating new workflows and developing new applications. Or you can use it while you are making ongoing improvements to existing applications and resources.

When you access the Trusted Advisor dashboard on the AWS Management Console, you can review completed checks for cost optimization, performance, security, fault tolerance, and service limits.

For each category:
------------------
--> The green check indicates the number of items for which it detected no problems.
--> The orange triangle represents the number of recommended investigations.
--> The red circle represents the number of recommended actions.

------------------------------------------------------------------------------------------------------------------------------------------------------------

Pricing:
--------

AWS Free Tier
-------------
The AWS Free Tier enables you to begin using certain services without having to worry about incurring costs for the specified period. 

Three types of offers are available: 

--> Always Free
--> 12 Months Free
--> Trials

For each free tier offer, make sure to review the specific details about exactly which resource types are included. 

Always Free
-----------
These offers do not expire and are available to all AWS customers.
Example: 
--> AWS Lambda allows 1 million free requests and up to 3.2 million seconds of compute time per month. 
--> Amazon DynamoDB allows 25 GB of free storage per month.

12 Months Free
--------------
These offers are free for 12 months following your initial sign-up date to AWS.
Example: 
--> Include specific amounts of Amazon S3 Standard Storage.
--> Thresholds for monthly hours of Amazon EC2 compute time
--> Amounts of Amazon CloudFront data transfer out.

Trials
------
--> Short-term free trial offers start from the date you activate a particular service. 
--> The length of each trial might vary by number of days or the amount of usage in the service.

Example:
--------
--> Amazon Inspector offers a 90-day free trial. 
--> Amazon Lightsail (a service that enables you to run virtual private servers) offers 750 free hours of usage over a 30-day period.

AWS Pricing Concepts
--------------------

How AWS Pricing Works ?
------------------------
AWS offers a range of cloud computing services with pay-as-you-go pricing. 

Pay for what you use
---------------------
For each service, you pay for exactly the amount of resources that you actually use, without requiring long-term contracts or complex licensing. 

Pay less when you reserve
-------------------------
Some services offer reservation options that provide a significant discount compared to On-Demand Instance pricing.

Example: Suppose that your company is using Amazon EC2 instances for a workload that needs to run continuously. You might choose to run this workload on Amazon EC2 Instance Savings Plans, because the plan allows you to save up to 72% over the equivalent On-Demand Instance capacity.

Pay less with volume-based discounts when you use more
------------------------------------------------------
Some services offer tiered pricing, so the per-unit cost is incrementally lower with increased usage.
Example: The more Amazon S3 storage space you use, the less you pay for it per GB.

AWS Pricing Calculator
----------------------
The AWS Pricing Calculator lets you explore AWS services and create an estimate for the cost of your use cases on AWS. You can organize your AWS estimates by groups that you define. A group can reflect how your company is organized, such as providing estimates by cost center.

AWS Lambda Pricing
-------------------
--> For AWS Lambda, you are charged based on the number of requests for your functions and the time that it takes for them to run.
--> AWS Lambda allows 1 million free requests and up to 3.2 million seconds of compute time per month.
--> You can save on AWS Lambda costs by signing up for a Compute Savings Plan. 
--> A Compute Savings Plan offers lower compute costs in exchange for committing to a consistent amount of usage over a 1-year or 3-year term. 
--> This is an example of paying less when you reserve. 

Amazon S3 Pricing
------------------
For Amazon S3 pricing, consider the following cost components:

Storage --> You pay for only the storage that you use. You are charged the rate to store objects in your Amazon S3 buckets based on your objects’ sizes, storage classes, and how long you have stored each object during the month.

Requests and data retrievals --> You pay for requests made to your Amazon S3 objects and buckets. For example, suppose that you are storing photo files in Amazon S3 buckets and hosting them on a website. Every time a visitor requests the website that includes these photo files, this counts towards requests you must pay for.

Data transfer --> There is no cost to transfer data between different Amazon S3 buckets or from Amazon S3 to other services within the same AWS Region. However, you pay for data that you transfer into and out of Amazon S3, with a few exceptions. There is no cost for data transferred into Amazon S3 from the internet or out to Amazon CloudFront. There is also no cost for data transferred out to an Amazon EC2 instance in the same AWS Region as the Amazon S3 bucket.

Management and replication --> You pay for the storage management features that you have enabled on your account’s Amazon S3 buckets. These features include Amazon S3 inventory, analytics, and object tagging.

Consolidated Billing
--------------------
--> The consolidated billing feature of AWS Organizations enables you to receive a single bill for all AWS accounts in your organization. 
--> By consolidating, you can easily track the combined costs of all the linked accounts in your organization. 
--> The default maximum number of accounts allowed for an organization is 4, but you can contact AWS Support to increase your quota, if needed.
--> On your monthly bill, you can review itemized charges incurred by each account. 
--> This enables greater transparency into your organization’s accounts while still maintaining the convenience of receiving a single monthly bill.
--> Another benefit of consolidated billing is the ability to share bulk discount pricing, Savings Plans, and Reserved Instances across the accounts. 
--> For instance, one account might not have enough monthly usage to qualify for discount pricing. 
--> However, when multiple accounts are combined, their aggregated usage may result in a benefit that applies across all accounts in the organization.

AWS Budgets
-----------
--> In AWS Budgets, you can create budgets to plan your service usage, service costs, and instance reservations.
--> The information in AWS Budgets updates three times a day. 
--> This helps you to accurately determine how close your usage is to your budgeted amounts or to the AWS Free Tier limits.
--> In AWS Budgets, you can also set custom alerts when your usage exceeds (or is forecasted to exceed) the budgeted amount.

AWS Cost Explorer
-----------------
--> AWS Cost Explorer is a tool that enables you to visualize, understand, and manage your AWS costs and usage over time.
--> AWS Cost Explorer includes a default report of the costs and usage for your top five cost-accruing AWS services. 
--> You can apply custom filters and groups to analyze your data. For example, you can view resource usage at the hourly level.

AWS Support
-----------
AWS offers four different Support plans to help you troubleshoot issues, lower costs, and efficiently use AWS services. 

You can choose from the following Support plans to meet your company’s needs: 
--> Basic
--> Developer
--> Business
--> Enterprise On-Ramp
--> Enterprise

Basic Support
-------------
--> Basic Support is free for all AWS customers. 
--> It includes access to whitepapers, documentation, and support communities. 
--> With Basic Support, you can also contact AWS for billing questions and service limit increases.
--> With Basic Support, you have access to a limited selection of AWS Trusted Advisor checks. 
--> You can use the AWS Personal Health Dashboard, a tool that provides alerts and remediation guidance when AWS is experiencing events that may affect you.

Developer, Business, Enterprise On-Ramp, and Enterprise Support
---------------------------------------------------------------
--> The Developer, Business, Enterprise On-Ramp, and Enterprise Support plans include all the benefits of Basic Support. 
--> In addition to the ability to open an unrestricted number of technical support cases. 
--> These three Support plans have pay-by-the-month pricing and require no long-term contracts. 

Developer Support
-----------------
--> Customers in the Developer Support plan have access to features such as:

Best practice guidance
----------------------
--> Client-side diagnostic tools
--> Building-block architecture support, which consists of guidance for how to use AWS offerings, features, and services together

Business Support
----------------
Customers with a Business Support plan have access to additional features, including: 
--> Use-case guidance to identify AWS offerings, features, and services that can best support your specific needs.
--> All AWS Trusted Advisor checks.
--> Limited support for third-party software, such as common operating systems and application stack components.

Enterprise On-Ramp Support 
---------------------------
In November 2021, AWS opened enrollment into AWS Enterprise On-Ramp Support plan. 
In addition to all the features included in the Basic, Developer, and Business Support plans,
Customers with an Enterprise On-Ramp Support plan have access to:

--> A pool of Technical Account Managers to provide proactive guidance and coordinate access to programs and AWS experts
--> A Cost Optimization workshop (one per year)
--> A Concierge support team for billing and account assistance
--> Tools to monitor costs and performance through Trusted Advisor and Health API/Dashboard
--> Consultative review and architecture guidance (one per year)
--> Infrastructure Event Management support (one per year)
--> Support automation workflows
--> 30 minutes or less response time for business-critical issues

Enterprise Support
------------------
--> In addition to all features included in the Basic, Developer, Business, and Enterprise On-Ramp support plans, 
Customers with Enterprise Support have access to:

--> A designated Technical Account Manager to provide proactive guidance and coordinate access to programs and AWS experts.
--> A Concierge support team for billing and account assistance.
--> Operations Reviews and tools to monitor health.
--> Training and Game Days to drive innovation.
--> Tools to monitor costs and performance through Trusted Advisor and Health API/Dashboard

Technical Account Manager (TAM)
-------------------------------
--> The Enterprise On-Ramp and Enterprise Support plans include access to a Technical Account Manager (TAM).
--> The TAM is your primary point of contact at AWS. 
--> Enterprise Support or Enterprise On-Ramp subscription gives TAM educates, empowers, and evolves the cloud journey across the full range of AWS services. 
--> TAMs provide expert engineering guidance, help you design solutions that efficiently integrate AWS services, assist with cost-effective and resilient architectures, and provide direct access to AWS programs and a broad community of experts.

AWS Marketplace
---------------
--> AWS Marketplace is a digital catalog that includes thousands of software listings from independent software vendors. 
--> One can use AWS Marketplace to find, test, and buy software that runs on AWS. 
--> For each listing in AWS Marketplace, you can access detailed information on pricing options, available support, and reviews from other AWS customers.

------------------------------------------------------------------------------------------------------------------------------------------------------------

Migration and Innovation
------------------------

AWS Cloud Adoption Framework (AWS CAF)
--------------------------------------
--> The AWS Cloud Adoption Framework (AWS CAF) organizes guidance into six areas of focus, called Perspectives. 
--> Each Perspective addresses distinct responsibilities. 
--> The planning process helps the right people across the organization prepare for the changes ahead.

--> In general, the Business, People, and Governance Perspectives focus on business capabilities.
--> Whereas the Platform, Security, and Operations Perspectives focus on technical capabilities.

Business Perspective
--------------------
--> The Business Perspective ensures that IT aligns with business needs and that IT investments link to key business results.
--> Use the Business Perspective to create a strong business case for cloud adoption and prioritize cloud adoption initiatives. 
--> Ensure that your business strategies and goals align with your IT strategies and goals.

Common roles in the Business Perspective include: 
--> Business managers
--> Finance managers
--> Budget owners
--> Strategy stakeholders

People Perspective
------------------
--> The People Perspective supports development of an organization-wide change management strategy for successful cloud adoption.
--> Use the People Perspective to evaluate organizational structures and roles, new skill and process requirements, and identify gaps. 
--> This helps prioritize training, staffing, and organizational changes.

Common roles in the People Perspective include: 
--> Human resources
--> Staffing
--> People managers

Governance Perspective
----------------------
--> The Governance Perspective focuses on the skills and processes to align IT strategy with business strategy. 
--> This ensures that you maximize the business value and minimize risks.
--> Use the Governance Perspective to understand how to update the staff skills and processes necessary to ensure business governance in the cloud. 
--> Manage and measure cloud investments to evaluate business outcomes.

Common roles in the Governance Perspective include: 
--> Chief Information Officer (CIO)
--> Program managers
--> Enterprise architects
--> Business analysts
--> Portfolio managers

Platform Perspective
--------------------
--> The Platform Perspective includes principles and patterns for implementing new solutions on the cloud, and migrating on-premises workloads to the cloud.
--> Use a variety of architectural models to understand and communicate the structure of IT systems and their relationships. 
--> Describe the architecture of the target state environment in detail.

Common roles in the Platform Perspective include: 
--> Chief Technology Officer (CTO)
--> IT managers
--> Solutions architects

Security Perspective
--------------------
--> The Security Perspective ensures that the organization meets security objectives for visibility, auditability, control, and agility. 
--> Use the AWS CAF to structure the selection and implementation of security controls that meet the organization’s needs.

Common roles in the Security Perspective include: 
--> Chief Information Security Officer (CISO)
--> IT security managers
--> IT security analysts

Operations Perspective
----------------------
--> The Operations Perspective helps you to enable, run, use, operate, and recover IT workloads to the level agreed upon with your business stakeholders.
--> Define how day-to-day, quarter-to-quarter, and year-to-year business is conducted. 
--> Align with and support the operations of the business. 
--> The AWS CAF helps these stakeholders define current operating procedures and identify the process changes and training needed to implement successful cloud adoption.

Common roles in the Operations Perspective include: 
--> IT operations managers
--> IT support managers

------------------------------------------------------------------------------------------------------------------------------------------------------------

Migration Stratergies:
----------------------
6 Strategies for Migration
---------------------------
When migrating applications to the cloud, six of the most common migration strategies that you can implement are:

--> Rehosting
--> Replatforming
--> Refactoring/re-architecting
--> Repurchasing
--> Retaining
--> Retiring

Rehosting
---------
--> Rehosting also known as “lift-and-shift” involves moving applications without changes. 
--> In the scenario of a large legacy migration, in which the company is looking to implement its migration and scale quickly to meet a business case, the majority of applications are rehosted. 

Replatforming
-------------
--> Replatforming, also known as “lift, tinker, and shift,” involves making a few cloud optimizations to realize a tangible benefit. 
--> Optimization is achieved without changing the core architecture of the application.

Refactoring/re-architecting
---------------------------
--> Refactoring (also known as re-architecting) involves reimagining how an application is architected and developed by using cloud-native features. 
--> Refactoring is driven by a strong business need to add features, scale, or performance that would otherwise be difficult to achieve in the application’s existing environment.

Repurchasing
------------
Repurchasing involves moving from a traditional license to a software-as-a-service model. 
For example, a business might choose to implement the repurchasing strategy by migrating from a customer relationship management (CRM) system to Salesforce.com.

Retaining
----------
--> Retaining consists of keeping applications that are critical for the business in the source environment. 
--> This might include applications that require major refactoring before they can be migrated, or, work that can be postponed until a later time.

Retiring
--------
Retiring is the process of removing applications that are no longer needed.

------------------------------------------------------------------------------------------------------------------------------------------------------------

AWS Snow Family
---------------

AWS Snow Family members
-----------------------
--> The AWS Snow Family is a collection of physical devices that help to physically transport up to exabytes of data into and out of AWS. 

AWS Snow Family is composed of:
--> AWS Snowcone
--> AWS Snowball
--> AWS Snowmobile

--> These devices offer different capacity points, and most include built-in computing capabilities. 
--> AWS owns and manages the Snow Family devices and integrates with AWS security, monitoring, storage management, and computing capabilities.  

AWS Snowcone:
-------------
--> AWS Snowcone(opens in a new tab) is a small, rugged, and secure edge computing and data transfer device. 
--> It features 2 CPUs, 4 GB of memory, and up to 14 TB of usable storage.

AWS Snowball:
-------------
AWS Snowball offers two types of devices:

1) Snowball Edge Storage Optimized devices:
-------------------------------------------
--> They are well suited for large-scale data migrations and recurring transfer workflows, in addition to local computing with higher capacity needs. 
--> Storage: 80 TB of hard disk drive (HDD) capacity for block volumes and Amazon S3 compatible object storage
--> 1 TB of SATA solid state drive (SSD) for block volumes. 
--> Compute: 40 vCPUs, and 80 GiB of memory to support Amazon EC2 sbe1 instances (equivalent to C5).

2) Snowball Edge Compute Optimized devices:
-------------------------------------------
--> They provides powerful computing resources for use cases such as machine learning, full motion video analysis, analytics, and local computing stacks. 
--> Storage: 80-TB usable HDD capacity for Amazon S3 compatible object storage or Amazon EBS compatible block volumes.
--> 28 TB of usable NVMe SSD capacity for Amazon EBS compatible block volumes. 
--> Compute: 104 vCPUs, 416 GiB of memory, and an optional NVIDIA Tesla V100 GPU. 
--> Devices run Amazon EC2 sbe-c and sbe-g instances, which are equivalent to C5, M5a, G3, and P3 instances.

AWS Snowmobile:
---------------
--> AWS Snowmobile is an exabyte-scale data transfer service used to move large amounts of data to AWS. 
--> You can transfer up to 100 petabytes of data per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi trailer truck.

------------------------------------------------------------------------------------------------------------------------------------------------------------

Innovate with AWS
-----------------

Serverless Applications
-----------------------
--> With AWS, serverless refers to applications that don’t require you to provision, maintain, or administer servers. 
--> You don’t need to worry about fault tolerance or availability. AWS handles these capabilities for you.
--> AWS Lambda is an example of a service that you can use to run serverless applications. 
--> If you design your architecture to trigger Lambda functions to run your code, you can bypass the need to manage a fleet of servers.
--> Building your architecture with serverless applications enables developers to focus on their core product instead of managing and operating servers.

Artificial Intelligence
-----------------------
--> AWS offers a variety of services powered by artificial intelligence (AI). 
For example, you can perform the following tasks:
--> Convert speech to text with Amazon Transcribe.
--> Discover patterns in text with Amazon Comprehend.
--> Identify potentially fraudulent online activities with Amazon Fraud Detector.
--> Build voice and text chatbots with Amazon Lex.

Machine Learning
-----------------
--> Traditional machine learning (ML) development is complex, expensive, time consuming, and error prone. 
--> AWS offers Amazon SageMaker to remove the difficult work from the process and empower you to build, train, and deploy ML models quickly.
--> You can use ML to analyze data, solve complex problems, and predict outcomes before they happen.

Which service enables you to quickly build, train, and deploy machine learning models?
--> Amazon Sagemaker

------------------------------------------------------------------------------------------------------------------------------------------------------------

The AWS Well-Architected Framework
----------------------------------
The AWS Well-Architected Framework helps you understand how to design and operate reliable, secure, efficient, and cost-effective systems in the AWS Cloud. It provides a way for you to consistently measure your architecture against best practices and design principles and identify areas for improvement.

The Well-Architected Framework is based on six pillars: 
--> Operational excellence
--> Security
--> Reliability
--> Performance efficiency
--> Cost optimization
--> Sustainability

Operational Excellence
----------------------
Operational excellence is the ability to run and monitor systems to deliver business value and to continually improve supporting processes and procedures.  
--> Design principles for operational excellence in the cloud include performing operations as code, annotating documentation, anticipating failure, and frequently making small, reversible changes.

Security
--------
The Security pillar is the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies. 

When considering the security of your architecture, apply these best practices:
--> Automate security best practices when possible.
--> Apply security at all layers.
--> Protect data in transit and at rest.

Reliability
-----------
Reliability is the ability of a system to do the following:

--> Recover from infrastructure or service disruptions
--> Dynamically acquire computing resources to meet demand
--> Mitigate disruptions such as misconfigurations or transient network issues
--> Reliability includes testing recovery procedures
--> Scaling horizontally to increase aggregate system availability, and automatically recovering from failure.

Performance Efficiency
----------------------
--> Performance efficiency is the ability to use computing resources efficiently to meet system requirements and to maintain that efficiency as demand changes and technologies evolve. 
--> Evaluating the performance efficiency of your architecture includes experimenting more often, using serverless architectures, and designing systems to be able to go global in minutes.

Cost Optimization
-----------------
--> Cost optimization is the ability to run systems to deliver business value at the lowest price point. 
--> Cost optimization includes adopting a consumption model, analyzing and attributing expenditure, and using managed services to reduce the cost of ownership.

Sustainability
--------------
--> In December 2021, AWS introduced a sustainability pillar as part of the AWS Well-Architected Framework.
--> Sustainability is the ability to continually improve sustainability impacts by reducing energy consumption and increasing efficiency across all components of a workload by maximizing the benefits from the provisioned resources and minimizing the total resources required.

------------------------------------------------------------------------------------------------------------------------------------------------------------

Benefits of the AWS Cloud
-------------------------
--> Trade upfront expense for variable expense.
--> Benefit from massive economies of scale.
--> Stop guessing capacity.
--> Increase speed and agility.
--> Stop spending money running and maintaining data centers.
--> Go global in minutes.

Trade upfront expense for variable expense
-------------------------------------------
Upfront expenses include data centers, physical servers, and other resources that you would need to invest in before using computing resources. 
Instead of investing heavily in data centers and servers before you know how you’re going to use them, you can pay only when you consume computing resources.

Benefit from massive economies of scale
----------------------------------------
--> By using cloud computing, you can achieve a lower variable cost than you can get on your own. 
--> Because usage from hundreds of thousands of customers aggregates in the cloud, providers such as AWS can achieve higher economies of scale. 
--> Economies of scale translate into lower pay-as-you-go prices.

Stop guessing capacity
----------------------
--> With cloud computing, you don’t have to predict how much infrastructure capacity you will need before deploying an application. 
--> For example, you can launch Amazon Elastic Compute Cloud (Amazon EC2) instances when needed and pay only for the compute time you use. 
--> Instead of paying for resources that are unused or dealing with limited capacity, you can access only the capacity that you need, and scale in 
or out in response to demand. 

Increase speed and agility
--------------------------
--> The flexibility of cloud computing makes it easier for you to develop and deploy applications.
--> This flexibility also provides your development teams with more time to experiment and innovate.

Stop spending money running and maintaining data centers
--------------------------------------------------------
--> Cloud computing in data centers often requires you to spend more money and time managing infrastructure and servers. 
--> A benefit of cloud computing is the ability to focus less on these tasks and more on your applications and customers.

Go global in minutes
---------------------
--> The AWS Cloud global footprint enables you to quickly deploy applications to customers around the world, while providing them with low latency.

------------------------------------------------------------------------------------------------------------------------------------------------------------


